{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14794836,"sourceType":"datasetVersion","datasetId":9459152}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"373798ce","cell_type":"markdown","source":"# Entity Extraction with **google/gemma-2-2b-it** (Kaggle Notebook)\nThis notebook extracts **ENTITIES ONLY** (no relationships) from `studium_llm_ready_people.jsonl` using the Hugging Face model **`google/gemma-2-2b-it`**.\n\nWhy entities-only first?\n- Easier to validate and iterate.\n- Lets you build an inventory of PEOPLE/PLACES/INSTITUTIONS/ROLES/WORKS before deciding relations.\n\n> ⚠️ Gemma is a **gated** Hugging Face repo → accept the license and authenticate with `HF_TOKEN` (Kaggle Secret) to avoid **401 Unauthorized**.\n\n## Output\nWrites JSONL to: `entity_outputs/entities_per_person.jsonl`  \nEach line contains: `reference`, `link`, and an `entities` list.\n","metadata":{}},{"id":"bfcb0550","cell_type":"markdown","source":"## Kaggle setup checklist\n- **GPU: ON**\n- **Internet: ON**\n- Add dataset containing `studium_llm_ready_people.jsonl`\n- Add Kaggle Secret:\n  - Name: `HF_TOKEN`\n  - Value: your Hugging Face token (after accepting Gemma license)\n","metadata":{}},{"id":"89c7d5a0","cell_type":"code","source":"# --- Configuration ---\nimport os\n\n# Update to your Kaggle dataset path:\nINPUT_JSONL = \"/kaggle/input/studium-llm2/studium_llm_ready_people.jsonl\"\n\n# Start small, then scale:\nLIMIT = 1  # 1 = dry run, 50/200 for pilot, None for full dataset\n\nMODEL_ID = \"google/gemma-2-2b-it\"\n\n# Deterministic generation\nMAX_NEW_TOKENS = 900\nTEMPERATURE = 0.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T10:41:21.457041Z","iopub.execute_input":"2026-02-12T10:41:21.457309Z","iopub.status.idle":"2026-02-12T10:41:21.464363Z","shell.execute_reply.started":"2026-02-12T10:41:21.457285Z","shell.execute_reply":"2026-02-12T10:41:21.463535Z"}},"outputs":[],"execution_count":1},{"id":"2b7a0447","cell_type":"code","source":"!pip -q install -U transformers accelerate pydantic tqdm huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:47:22.868130Z","iopub.execute_input":"2026-02-10T10:47:22.869114Z","iopub.status.idle":"2026-02-10T10:47:42.534461Z","shell.execute_reply.started":"2026-02-10T10:47:22.869077Z","shell.execute_reply":"2026-02-10T10:47:42.533157Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.3/553.3 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 5.1.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"id":"92e72ab1","cell_type":"code","source":"import json, re, time\nfrom typing import List, Optional, Literal, Dict, Any\nfrom pydantic import BaseModel, Field, field_validator\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T10:41:24.521129Z","iopub.execute_input":"2026-02-12T10:41:24.521416Z","iopub.status.idle":"2026-02-12T10:41:24.791176Z","shell.execute_reply.started":"2026-02-12T10:41:24.521384Z","shell.execute_reply":"2026-02-12T10:41:24.790428Z"}},"outputs":[],"execution_count":2},{"id":"00a96a69","cell_type":"markdown","source":"## Authenticate to Hugging Face (required for Gemma)","metadata":{}},{"id":"da388c61","cell_type":"code","source":"# from huggingface_hub import login\n# import os\n\n# HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n# if not HF_TOKEN:\n#     raise RuntimeError(\"HF_TOKEN not found. Add it in Kaggle Secrets as HF_TOKEN, then restart the session.\")\n\n# login(token=HF_TOKEN)\n# print(\"✅ Logged in to Hugging Face.\")\n\nfrom huggingface_hub import login\nimport os\n\nlogin(token=\"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\")\nprint(\"Logged in to HuggingFace.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T10:41:27.160997Z","iopub.execute_input":"2026-02-12T10:41:27.161560Z","iopub.status.idle":"2026-02-12T10:41:27.752119Z","shell.execute_reply.started":"2026-02-12T10:41:27.161532Z","shell.execute_reply":"2026-02-12T10:41:27.751351Z"}},"outputs":[{"name":"stdout","text":"Logged in to HuggingFace.\n","output_type":"stream"}],"execution_count":3},{"id":"0302d5c3","cell_type":"markdown","source":"## Load Gemma 2B Instruct","metadata":{}},{"id":"13f1a318","cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    device_map=\"auto\",\n    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n)\nprint(\"✅ Loaded:\", MODEL_ID, \"| GPU:\", torch.cuda.is_available())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T10:41:31.045693Z","iopub.execute_input":"2026-02-12T10:41:31.046189Z","iopub.status.idle":"2026-02-12T10:42:20.912893Z","shell.execute_reply.started":"2026-02-12T10:41:31.046162Z","shell.execute_reply":"2026-02-12T10:42:20.912229Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66d4dab5c95245cda5148837dee76dd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"108e8c7146ae4779afb157d1b6304de8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eeb1a6492c341e2bce07514abbaec4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14070e59d4fe415bbb59aa825c93346b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6050fabddcb549f5b5ab64ce0be47364"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n2026-02-12 10:41:47.240911: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770892907.429410      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770892907.484245      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770892907.935965      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770892907.935996      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770892907.935998      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770892907.936001      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d53c5fa0885343b58cb785c0acddf595"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca240ef3754c40028f6777b04896687a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3606aaafba504df68d7ecd6b00c960e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77b73fa66933473cb9f29a438c428da9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf284f722a674425beff8015990f571d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e1c43818ef94b2dbba4ce494a7a0345"}},"metadata":{}},{"name":"stdout","text":"✅ Loaded: google/gemma-2-2b-it | GPU: True\n","output_type":"stream"}],"execution_count":4},{"id":"8f1ba5d9","cell_type":"code","source":"# --- IO helpers ---\ndef iter_jsonl(path: str):\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            yield json.loads(line)\n\ndef clip(s: str, n: int = 200) -> str:\n    s = (s or \"\").strip()\n    return s if len(s) <= n else s[:n] + \"…\"\n\ndef slugify(s: str) -> str:\n    s = (s or \"\").strip().lower()\n    s = re.sub(r\"['’]\", \"\", s)\n    s = re.sub(r\"[^a-z0-9]+\", \"-\", s)\n    s = re.sub(r\"-{2,}\", \"-\", s).strip(\"-\")\n    return s or \"unknown\"\n\ndef extract_first_json(text: str) -> Optional[str]:\n    if not text:\n        return None\n    start = text.find(\"{\")\n    end = text.rfind(\"}\")\n    if start == -1 or end == -1 or end <= start:\n        return None\n    return text[start:end+1].strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T10:42:31.454664Z","iopub.execute_input":"2026-02-12T10:42:31.455417Z","iopub.status.idle":"2026-02-12T10:42:31.463823Z","shell.execute_reply.started":"2026-02-12T10:42:31.455384Z","shell.execute_reply":"2026-02-12T10:42:31.463223Z"}},"outputs":[],"execution_count":5},{"id":"57e77e6d","cell_type":"markdown","source":"## Strict entity schema (validated)\nWe only extract **nodes** (entities).\n\nEntity types:\n- PERSON, PLACE, INSTITUTION, ROLE, WORK, GROUP, EVENT, MANUSCRIPT, DATE, VALUE, OTHER\n\nStable IDs:\n- PERSON:ref:<reference>\n- PLACE:<slug>, INSTITUTION:<slug>, ROLE:<slug>, WORK:<slug>, GROUP:<slug>, EVENT:<slug>\n- DATE:<yyyy> or DATE:<yyyy-yyyy>\n- VALUE:<slug>\n","metadata":{}},{"id":"c4db9a90","cell_type":"code","source":"EntityType = Literal[\n    \"PERSON\",\"PLACE\",\"INSTITUTION\",\"ROLE\",\"WORK\",\"GROUP\",\"EVENT\",\"MANUSCRIPT\",\"DATE\",\"VALUE\",\"OTHER\"\n]\n\nclass KGEntity(BaseModel):\n    entity_id: str\n    type: EntityType\n    name: str\n    properties: Dict[str, Any] = Field(default_factory=dict)\n\n    @field_validator(\"entity_id\")\n    @classmethod\n    def id_has_colon(cls, v):\n        v = v.strip()\n        if \":\" not in v:\n            raise ValueError(\"entity_id must contain ':'\")\n        return v\n\nclass EntitiesOnlyOutput(BaseModel):\n    reference: str\n    link: Optional[str] = None\n    entities: List[KGEntity]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T10:42:35.187207Z","iopub.execute_input":"2026-02-12T10:42:35.187502Z","iopub.status.idle":"2026-02-12T10:42:35.236494Z","shell.execute_reply.started":"2026-02-12T10:42:35.187476Z","shell.execute_reply":"2026-02-12T10:42:35.235930Z"}},"outputs":[],"execution_count":6},{"id":"57ff188c","cell_type":"markdown","source":"## Prompt (entities-only, high recall)\nFocus: extract as many *useful nodes* as possible **without hallucinating**.\n\nWe require:\n- PERSON node always\n- extract PLACE / INSTITUTION / ROLE / WORK / DATE / VALUE nodes when they appear\n- deduplicate by entity_id\n- store sources/comments in `properties` if present\n","metadata":{}},{"id":"14405c2f","cell_type":"code","source":"def build_entities_prompt(person: dict) -> str:\n    ref = str(person.get(\"reference\",\"\")).strip()\n    link = (person.get(\"link\") or person.get(\"url\") or \"\").strip()\n    name = (person.get(\"name\") or person.get(\"title\") or \"\").strip()\n    text = (person.get(\"text\") or \"\").strip()\n\n    return f\"\"\"You are a strict ENTITY EXTRACTION system.\nExtract entities from the TEXT and return ONLY valid JSON (no markdown, no extra text) matching EXACTLY:\n{{\n  \"reference\": \"{ref}\",\n  \"link\": \"{link}\",\n  \"entities\": [\n    {{\"entity_id\":\"...\",\"type\":\"PERSON|PLACE|INSTITUTION|ROLE|WORK|GROUP|EVENT|MANUSCRIPT|DATE|VALUE|OTHER\",\"name\":\"...\",\"properties\":{{}}}},\n    ...\n  ]\n}}\n\nRULES (must follow):\n1) Do NOT guess. Only extract entities explicitly supported by TEXT.\n2) ALWAYS include the main PERSON entity:\n   {{\"entity_id\":\"PERSON:ref:{ref}\",\"type\":\"PERSON\",\"name\":\"{name}\",\"properties\":{{}}}}\n3) Use stable IDs and slugify:\n   - PLACE:<slug> (example: PLACE:paris)\n   - INSTITUTION:<slug>\n   - ROLE:<slug>\n   - WORK:<slug>\n   - GROUP:<slug>\n   - EVENT:<slug>\n   - MANUSCRIPT:<slug>\n   - DATE:<yyyy> or DATE:<yyyy-yyyy>\n   - VALUE:<slug> for categorical values (male, maître, degrees) if present in TEXT\n4) DEDUPLICATE: do not output two entities with the same entity_id.\n5) For curriculum: if you see a city like Paris, create PLACE:paris.\n   If it is clearly a university/institution, also create INSTITUTION:paris (or INSTITUTION:university-of-paris if explicitly stated).\n6) Put sources/comments into properties when present (e.g., {{\"source\":\"FOURNIER: 2, 5\"}}).\n\nWHAT TO EXTRACT (priority):\nA) Places (cities, regions, dioceses)\nB) Institutions (universities, colleges, churches, bishoprics)\nC) Roles / degrees / titles (as ROLE or VALUE nodes)\nD) Works (if explicit)\nE) Dates (activity/life years or intervals)\n\nReturn JSON only.\n\nTEXT:\n{text}\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T10:42:40.302334Z","iopub.execute_input":"2026-02-12T10:42:40.302972Z","iopub.status.idle":"2026-02-12T10:42:40.308841Z","shell.execute_reply.started":"2026-02-12T10:42:40.302937Z","shell.execute_reply":"2026-02-12T10:42:40.307899Z"}},"outputs":[],"execution_count":7},{"id":"f29765c7","cell_type":"code","source":"def call_gemma(prompt: str) -> str:\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    import torch\n    with torch.no_grad():\n        out = model.generate(\n            **inputs,\n            max_new_tokens=MAX_NEW_TOKENS,\n            do_sample=False,\n            temperature=TEMPERATURE,\n            eos_token_id=tokenizer.eos_token_id,\n        )\n    full = tokenizer.decode(out[0], skip_special_tokens=True)\n    if full.startswith(prompt):\n        full = full[len(prompt):]\n    return full.strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T10:42:45.008997Z","iopub.execute_input":"2026-02-12T10:42:45.009550Z","iopub.status.idle":"2026-02-12T10:42:45.014224Z","shell.execute_reply.started":"2026-02-12T10:42:45.009518Z","shell.execute_reply":"2026-02-12T10:42:45.013561Z"}},"outputs":[],"execution_count":8},{"id":"398b19ec","cell_type":"markdown","source":"## Parse + repair (one retry)","metadata":{}},{"id":"8233274c","cell_type":"code","source":"def parse_entities(raw: str) -> EntitiesOnlyOutput:\n    j = extract_first_json(raw) or raw\n    data = json.loads(j)\n    return EntitiesOnlyOutput.model_validate(data)\n\ndef repair_json(bad: str, err: str) -> str:\n    repair_prompt = f\"\"\"Rewrite into VALID JSON ONLY matching the required schema.\nNo extra text.\n\nValidation error:\n{err}\n\nBad output:\n{bad}\n\"\"\"\n    return call_gemma(repair_prompt)\n\ndef extract_entities_one(person: dict) -> EntitiesOnlyOutput:\n    prompt = build_entities_prompt(person)\n    raw = call_gemma(prompt)\n    try:\n        out = parse_entities(raw)\n    except Exception as e:\n        fixed = repair_json(raw, str(e))\n        out = parse_entities(fixed)\n\n    # Dedup by entity_id\n    seen = set()\n    dedup = []\n    for ent in out.entities:\n        if ent.entity_id in seen:\n            continue\n        seen.add(ent.entity_id)\n        dedup.append(ent)\n    out.entities = dedup\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T10:43:07.883205Z","iopub.execute_input":"2026-02-12T10:43:07.884016Z","iopub.status.idle":"2026-02-12T10:43:07.889739Z","shell.execute_reply.started":"2026-02-12T10:43:07.883985Z","shell.execute_reply":"2026-02-12T10:43:07.888978Z"}},"outputs":[],"execution_count":10},{"id":"047488e1","cell_type":"markdown","source":"## Dry run: 1 example\nShows the prompt preview + extracted entities.\n","metadata":{}},{"id":"4ae0f8f2","cell_type":"code","source":"first = next(iter_jsonl(INPUT_JSONL))\nprompt = build_entities_prompt(first)\n\nprint(\"=== PROMPT PREVIEW (first 1200 chars) ===\")\nprint(prompt[:1200] + (\"...\" if len(prompt) > 1200 else \"\"))\n\nout = extract_entities_one(first)\nprint(\"\\n=== ENTITIES OUTPUT (pretty JSON, truncated) ===\")\nprint(json.dumps(out.model_dump(), ensure_ascii=False, indent=2)[:3500])\n\nprint(\"\\nEntity count:\", len(out.entities))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:50:58.509289Z","iopub.execute_input":"2026-02-10T10:50:58.510206Z","iopub.status.idle":"2026-02-10T10:55:18.723321Z","shell.execute_reply.started":"2026-02-10T10:50:58.510167Z","shell.execute_reply":"2026-02-10T10:55:18.722125Z"}},"outputs":[{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"=== PROMPT PREVIEW (first 1200 chars) ===\nYou are a strict ENTITY EXTRACTION system.\nExtract entities from the TEXT and return ONLY valid JSON (no markdown, no extra text) matching EXACTLY:\n{\n  \"reference\": \"15657\",\n  \"link\": \"http://studium-parisiense.univ-paris1.fr/individus/15657-ancelinusgalli\",\n  \"entities\": [\n    {\"entity_id\":\"...\",\"type\":\"PERSON|PLACE|INSTITUTION|ROLE|WORK|GROUP|EVENT|MANUSCRIPT|DATE|VALUE|OTHER\",\"name\":\"...\",\"properties\":{}},\n    ...\n  ]\n}\n\nRULES (must follow):\n1) Do NOT guess. Only extract entities explicitly supported by TEXT.\n2) ALWAYS include the main PERSON entity:\n   {\"entity_id\":\"PERSON:ref:15657\",\"type\":\"PERSON\",\"name\":\"ANCELINUS Galli\",\"properties\":{}}\n3) Use stable IDs and slugify:\n   - PLACE:<slug> (example: PLACE:paris)\n   - INSTITUTION:<slug>\n   - ROLE:<slug>\n   - WORK:<slug>\n   - GROUP:<slug>\n   - EVENT:<slug>\n   - MANUSCRIPT:<slug>\n   - DATE:<yyyy> or DATE:<yyyy-yyyy>\n   - VALUE:<slug> for categorical values (male, maître, degrees) if present in TEXT\n4) DEDUPLICATE: do not output two entities with the same entity_id.\n5) For curriculum: if you see a city like Paris, create PLACE:paris.\n   If it is clearly a university/institution, also create INSTITUTION:paris (or INSTITUTION:universi...\n\n=== ENTITIES OUTPUT (pretty JSON, truncated) ===\n{\n  \"reference\": \"15657\",\n  \"link\": \"http://studium-parisiense.univ-paris1.fr/individus/15657-ancelinusgalli\",\n  \"entities\": [\n    {\n      \"entity_id\": \"PERSON:ref:15657\",\n      \"type\": \"PERSON\",\n      \"name\": \"ANCELINUS Galli\",\n      \"properties\": {\n        \"source\": \"FOURNIER: 2, 5\"\n      }\n    },\n    {\n      \"entity_id\": \"PLACE:paris\",\n      \"type\": \"PLACE\",\n      \"name\": \"Paris\",\n      \"properties\": {}\n    },\n    {\n      \"entity_id\": \"INSTITUTION:university-of-paris\",\n      \"type\": \"INSTITUTION\",\n      \"name\": \"University of Paris\",\n      \"properties\": {\n        \"source\": \"FOURNIER: 2, 5\"\n      }\n    },\n    {\n      \"entity_id\": \"WORK:Maître ès arts\",\n      \"type\": \"WORK\",\n      \"name\": \"Maître ès arts\",\n      \"properties\": {\n        \"source\": \"FOURNIER: 2, 5\"\n      }\n    },\n    {\n      \"entity_id\": \"WORK:Bachelier en décret\",\n      \"type\": \"WORK\",\n      \"name\": \"Bachelier en décret\",\n      \"properties\": {\n        \"source\": \"FOURNIER: 2, 5\"\n      }\n    },\n    {\n      \"entity_id\": \"DATE:1435\",\n      \"type\": \"DATE\",\n      \"name\": \"1435\",\n      \"properties\": {}\n    }\n  ]\n}\n\nEntity count: 6\n","output_type":"stream"}],"execution_count":15},{"id":"cee5fac9-59ca-47bb-9b95-f7d7354f4b8a","cell_type":"code","source":"first = next(iter_jsonl(INPUT_JSONL))\nprompt = build_entities_prompt(first)\n\nprint(\"=== PROMPT PREVIEW (first 1200 chars) ===\")\nprint(prompt[:1200] + (\"...\" if len(prompt) > 1200 else \"\"))\n\nout = extract_entities_one(first)\nprint(\"\\n=== ENTITIES OUTPUT (pretty JSON, truncated) ===\")\nprint(json.dumps(out.model_dump(), ensure_ascii=False, indent=2)[:3500])\n\nprint(\"\\nEntity count:\", len(out.entities))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T10:43:27.797958Z","iopub.execute_input":"2026-02-12T10:43:27.798283Z","iopub.status.idle":"2026-02-12T10:43:46.800563Z","shell.execute_reply.started":"2026-02-12T10:43:27.798255Z","shell.execute_reply":"2026-02-12T10:43:46.799896Z"}},"outputs":[{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"=== PROMPT PREVIEW (first 1200 chars) ===\nYou are a strict ENTITY EXTRACTION system.\nExtract entities from the TEXT and return ONLY valid JSON (no markdown, no extra text) matching EXACTLY:\n{\n  \"reference\": \"15657\",\n  \"link\": \"http://studium-parisiense.univ-paris1.fr/individus/15657-ancelinusgalli\",\n  \"entities\": [\n    {\"entity_id\":\"...\",\"type\":\"PERSON|PLACE|INSTITUTION|ROLE|WORK|GROUP|EVENT|MANUSCRIPT|DATE|VALUE|OTHER\",\"name\":\"...\",\"properties\":{}},\n    ...\n  ]\n}\n\nRULES (must follow):\n1) Do NOT guess. Only extract entities explicitly supported by TEXT.\n2) ALWAYS include the main PERSON entity:\n   {\"entity_id\":\"PERSON:ref:15657\",\"type\":\"PERSON\",\"name\":\"ANCELINUS Galli\",\"properties\":{}}\n3) Use stable IDs and slugify:\n   - PLACE:<slug> (example: PLACE:paris)\n   - INSTITUTION:<slug>\n   - ROLE:<slug>\n   - WORK:<slug>\n   - GROUP:<slug>\n   - EVENT:<slug>\n   - MANUSCRIPT:<slug>\n   - DATE:<yyyy> or DATE:<yyyy-yyyy>\n   - VALUE:<slug> for categorical values (male, maître, degrees) if present in TEXT\n4) DEDUPLICATE: do not output two entities with the same entity_id.\n5) For curriculum: if you see a city like Paris, create PLACE:paris.\n   If it is clearly a university/institution, also create INSTITUTION:paris (or INSTITUTION:universi...\n\n=== ENTITIES OUTPUT (pretty JSON, truncated) ===\n{\n  \"reference\": \"15657\",\n  \"link\": \"http://studium-parisiense.univ-paris1.fr/individus/15657-ancelinusgalli\",\n  \"entities\": [\n    {\n      \"entity_id\": \"PERSON:ref:15657\",\n      \"type\": \"PERSON\",\n      \"name\": \"ANCELINUS Galli\",\n      \"properties\": {\n        \"source\": \"FOURNIER: 2, 5\"\n      }\n    },\n    {\n      \"entity_id\": \"PLACE:paris\",\n      \"type\": \"PLACE\",\n      \"name\": \"Paris\",\n      \"properties\": {}\n    },\n    {\n      \"entity_id\": \"INSTITUTION:university-of-paris\",\n      \"type\": \"INSTITUTION\",\n      \"name\": \"University of Paris\",\n      \"properties\": {\n        \"source\": \"FOURNIER: 2, 5\"\n      }\n    },\n    {\n      \"entity_id\": \"WORK:Maître ès arts\",\n      \"type\": \"WORK\",\n      \"name\": \"Maître ès arts\",\n      \"properties\": {}\n    }\n  ]\n}\n\nEntity count: 4\n","output_type":"stream"}],"execution_count":11},{"id":"d6264974","cell_type":"markdown","source":"## Batch extraction (optional)\nWrites to `entity_outputs/entities_per_person.jsonl`.\nStart with LIMIT=50 to validate quality before scaling.\n","metadata":{}},{"id":"b99ddc0d","cell_type":"code","source":"from tqdm import tqdm\nimport os\n\nLIMIT = 3\nOUT_DIR = \"entity_outputs\"\nos.makedirs(OUT_DIR, exist_ok=True)\nOUT_JSONL = os.path.join(OUT_DIR, \"entities_per_person.jsonl\")\n\nif os.path.exists(OUT_JSONL):\n    os.remove(OUT_JSONL)\n\npeople = list(iter_jsonl(INPUT_JSONL))\nif LIMIT is not None:\n    people = people[:LIMIT]\n\nfailed = []\nwith open(OUT_JSONL, \"a\", encoding=\"utf-8\") as f:\n    for person in tqdm(people, desc=\"Extracting entities\"):\n        try:\n            out = extract_entities_one(person)\n            f.write(json.dumps(out.model_dump(), ensure_ascii=False) + \"\\n\")\n        except Exception as e:\n            failed.append({\"reference\": str(person.get(\"reference\",\"\")), \"error\": str(e)[:800]})\n\nprint(\"Done. Output:\", OUT_JSONL)\nprint(\"Failed:\", len(failed))\nif failed:\n    print(\"First failure:\", failed[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T10:44:30.885304Z","iopub.execute_input":"2026-02-12T10:44:30.885619Z","iopub.status.idle":"2026-02-12T10:46:08.802033Z","shell.execute_reply.started":"2026-02-12T10:44:30.885592Z","shell.execute_reply":"2026-02-12T10:46:08.801317Z"}},"outputs":[{"name":"stderr","text":"Extracting entities: 100%|██████████| 3/3 [01:36<00:00, 32.09s/it]","output_type":"stream"},{"name":"stdout","text":"Done. Output: entity_outputs/entities_per_person.jsonl\nFailed: 0\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":12}]}